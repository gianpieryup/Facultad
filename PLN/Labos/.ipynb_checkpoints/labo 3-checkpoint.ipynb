{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d31f93f5",
   "metadata": {},
   "source": [
    "082057 – Procesamiento del Lenguaje Natural\n",
    "<center><h1>Lab 3 </h1> </center>\n",
    "<center><h2>Procesando Texto y usando scikit-learn</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa06e4f0",
   "metadata": {},
   "source": [
    "### 2.1 El uso de SciKit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c0b376c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a936ff2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = np.array([[1,2], [2,3], [3,4], [4,5], [5,6]])\n",
    "x = data[:,0]\n",
    "y= data[:,1]\n",
    "\n",
    "#Ahora puede visualizar la función de la siguiente manera:\n",
    "plt.scatter(x,y)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5c4783",
   "metadata": {},
   "source": [
    "### 2.2 Preprocesamiento de Texto con SciKit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bed692d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94f5f509",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = [\"How to format my hard disk\", \" Hard disk format problems \"]\n",
    "X = vectorizer.fit_transform(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10eee979",
   "metadata": {},
   "source": [
    "**fit_transform** ha extraído siete características de los dos “documentos”;\n",
    "podemos ver eso con el método **get_feature_names()**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5ffd3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GIANPIER\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['disk', 'format', 'hard', 'how', 'my', 'problems', 'to']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3701b188",
   "metadata": {},
   "source": [
    "Se puede ver cuántas veces cada una de estas siete features se produce en los dos documentos haciendo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57e236e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 0, 1],\n",
       "       [1, 1, 1, 0, 0, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a61a93f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Donde el resultado es el numero de veces que aparecio 'hard' en el segundo documento\n",
    "X.toarray()[1,2] # [segundo documento, 'hard']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d52094b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics','sci.med']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb72766",
   "metadata": {},
   "source": [
    "Podemos importar los documentos pertenecientes a las categorías de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b6b965f",
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295af851",
   "metadata": {},
   "source": [
    "(La primera vez que se hace esto, se va a tomar un tiempo, no te preocupes. \n",
    "También es posible que puedas recibir un mensaje de alerta acerca de no encontrar los controladores, ignoralo)\n",
    "Los archivos han sido cargados en el atributo **data** del objeto **twenty_train**\n",
    "\n",
    "Vamos ahora a crear un nuevo objeto CountVectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7094664a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31431e01",
   "metadata": {},
   "source": [
    "Una vez más, la función fit_transform se puede utilizar para tokenizar cada documento, identificar las palabras más relevantes, construir un diccionario de tales palabras, y crear para cada documento una representación vectorial en el que las palabras son las features y el valor de estas características es el número de\n",
    "ocurrencias de cada palabra en un documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84d87141",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_counts = vectorizer.fit_transform(twenty_train.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d27e78d",
   "metadata": {},
   "source": [
    "Por ejemplo, si ahora queremos ver la frecuencia de la palabra 'algorithm'\n",
    "se produce en el subconjunto de la colección 20Newgroups estamos\n",
    "considerando la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "21938b69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4690"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_.get('algorithm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8358de26",
   "metadata": {},
   "source": [
    "Para ver cuántos términos fueron extraídos, podemos utilizar get_feature_names ()\n",
    "que hemos visto anteriormente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1cf947ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35788"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a43c67a",
   "metadata": {},
   "source": [
    "La clase CountVectorizer de SciKit-learn puede **hacer más\n",
    "procesamiento previo** de una colección de documentos que simples\n",
    "tokenizaciones.\n",
    "\n",
    "Una importante etapa de preprocesamiento adicional de que la clase\n",
    "puede llevar a cabo es la **eliminación de stop words**.\n",
    "\n",
    "Esto se puede hacer mediante la especificación de un parámetros de\n",
    "CountVectorizer, como sigue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d5adea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Vectorizer = CountVectorizer(stop_words = 'english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c15fe03",
   "metadata": {},
   "source": [
    "(Pista: si ponen **spanish** considerará las **stop words** o *palabras más comunes y\n",
    "puntuaciones del español, y así con otros idiomas*)\n",
    "\n",
    "Para ver qué palabras son palabras stop words, hace lo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c8e9f75a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'across',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amoungst']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(Vectorizer.get_stop_words())[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a07b235",
   "metadata": {},
   "source": [
    "### 3. Pre-procesamiento más avanzado con NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7150fc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df21949f",
   "metadata": {},
   "source": [
    "#### Steamming\n",
    "\n",
    "Llevar una palabra a su raiz. Perros -> **perro** | Amorozo -> **amor**\n",
    "\n",
    "El steamming en NLTK incluye implementaciones de varios algoritmos\n",
    "muy conocidos y utilizados, incluyendo el Porter Stemmer y el Lancaster Stemmer.\n",
    "\n",
    "Para crear un steammer de Inglés que tiene que hacer lo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c8e3b8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = nltk.stem.SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6694c36b",
   "metadata": {},
   "source": [
    "Después de crear el steammer, a continuación, puede utilizarlo para llevar a la\n",
    "raíz (steam) palabras de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e37fd289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cat'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.stem(\"cats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "509346f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.stem(\"loving\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14e09b7",
   "metadata": {},
   "source": [
    "Otros tipos de pre-procesamiento de NLTK incluye implementaciones de\n",
    "muchos de los módulos de procesamiento previo y analizadores\n",
    "sintácticos que discutimos o discutiremos en las clases:\n",
    "- identificadores de idioma\n",
    "- tokenizers para varios idiomas\n",
    "- divisores de oraciones\n",
    "- POS taggers\n",
    "- Chunkers\n",
    "- Parsers\n",
    "\n",
    "Además, NLTK incluye implementaciones de los aspectos del análisis de texto\n",
    "que vamos a discutir en este módulo, incluyendo\n",
    "- NER (Named Entity Recognition)\n",
    "- Análisis de los sentimientos\n",
    "- Extraer información de los medios de redes sociales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fb39b7",
   "metadata": {},
   "source": [
    "Por ejemplo, las instrucciones siguientes ( puede que tengas que descargar el paquete\n",
    "NLTK 'punkt' para hacer esto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8b79e024",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "#nltk.download(‘punkt’) #if not used nltk.download()    Descargue todo el NLTK en la primer lab\n",
    "text = word_tokenize(\"And now for something completely different\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5225d7e1",
   "metadata": {},
   "source": [
    "producir una versión tokenizada de la frase, que luego puede ser alimentado en el\n",
    "etiquetador POS ( puede que tenga que descargar el paquete ' maxent_...' para hacer\n",
    "esto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d47dbb4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('And', 'CC'),\n",
       " ('now', 'RB'),\n",
       " ('for', 'IN'),\n",
       " ('something', 'NN'),\n",
       " ('completely', 'RB'),\n",
       " ('different', 'JJ')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea0dda7",
   "metadata": {},
   "source": [
    "### 4. La integración el steammer de NLTK con el CountVectorizer de SciKit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223802f5",
   "metadata": {},
   "source": [
    "El steammer de NLTK puede ser utilizado antes de la alimentación en CountVectorizer\n",
    "de SciKit-learn, obteniendo así un índice más compacto.\n",
    "\n",
    "Una forma de hacer esto es definir una nueva clase StemmedCountVectorizer\n",
    "Extendiendo de CountVectorizer y redefiniendo el método **build_analyzer()**\n",
    "que se encarga de pre-procesamiento y tokenización:\n",
    "\n",
    "**build_analyzer()** toma un string como entrada y como salida una lista de tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "268c75ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['john', 'bought', 'carrots', 'potatoes']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "analyze = vectorizer.build_analyzer()\n",
    "analyze(\"John bought carrots and potatoes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f483642d",
   "metadata": {},
   "source": [
    "Si modificamos **build_analyzer()** para aplicar el steammer de NLTK a la salida del\n",
    "método **build_analyzer()**, obtenemos una versión que deriva así:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f79f3ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.stem\n",
    "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(, self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3f03f6",
   "metadata": {},
   "source": [
    "Ahora Podemos crear una instancia de nuestra clase!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b33393c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_vectorizer = StemmedCountVectorizer(min_df=1, stop_words='english')\n",
    "stem_analyze = stem_vectorizer.build_analyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d0cdff",
   "metadata": {},
   "source": [
    "como se puede ver, estos nuevos usos Vectorizer surgieron versiones de fichas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "901cc6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "john\n",
      "bought\n",
      "carrot\n",
      "potato\n"
     ]
    }
   ],
   "source": [
    "Y = stem_analyze(\"John bought carrots and potatoes\")\n",
    "for tok in Y:\n",
    "    print(tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a653c46",
   "metadata": {},
   "source": [
    "Vemos que al resultado anterior le aplico el streammer (osea que los dejo en la raiz)\n",
    "\n",
    "Si utilizamos este Vectorizer para extraer features para el subconjunto del\n",
    "dataset **20_Newsgroups** que consideramos antes, vamos a tener un menor\n",
    "número de **features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3d69a5bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26888"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.datasets import fetch_20newsgroups   # En las celdas pasadas se importo\n",
    "categories = ['alt.atheism','soc.religion.christian','comp.graphics', 'sci.med']\n",
    "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
    "train_counts = stem_vectorizer.fit_transform(twenty_train.data)\n",
    "len(stem_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f31a11",
   "metadata": {},
   "source": [
    "(Compará este número con los alrededor de 35.000 características que hemos obtenido\n",
    "usando la **versión sin hacer steam**).\n",
    "\n",
    "Claro tendremos mas porque **no estan stemiados** (osea que hay nadar, nadare, nadando , nade, etc) y en el conjunto de los **steam** solo nadar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a7b6e7",
   "metadata": {},
   "source": [
    "### EJERCITACION\n",
    "\n",
    "Subir a tu github una implementación personalizada de NLTK para CountVectorizer\n",
    "que haga steam y stopwords del idioma español y dos ejemplos de oraciones\n",
    "usando tu clase.\n",
    "También importá un corpus como 20_Newsdataset pero que esté en español. Qué\n",
    "corpus poner, queda a tu criterio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "36acf1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.stem\n",
    "spanish_stemmer=nltk.stem.SnowballStemmer('spanish')\n",
    "class GianpierStemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "           analyzer=super(GianpierStemmedCountVectorizer,self).build_analyzer()\n",
    "           return lambda doc: (spanish_stemmer.stem(w) for w in analyzer(doc))\n",
    "    \n",
    "stem_vectorizer = GianpierStemmedCountVectorizer(min_df=1, stop_words=stopwords.words('spanish'))\n",
    "stem_analyze = stem_vectorizer.build_analyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "93dc074c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "juan\n",
      "compr\n",
      "zanahori\n",
      "pap\n"
     ]
    }
   ],
   "source": [
    "Y = stem_analyze(\"Juan compro zanahorias y papas\")\n",
    "for tok in Y:\n",
    "    print(tok) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1da27791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gianpi\n",
      "compr\n",
      "hel\n",
      "pap\n",
      "frit\n"
     ]
    }
   ],
   "source": [
    "Z = stem_analyze(\"Gianpier compro helados y papas fritas\")\n",
    "for tok in Z:\n",
    "    print(tok) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "261634a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('EDF', 'np00000'), ('tiene', 'vmip3s0'), ('previsto', 'aq0msp'), ('invertir', 'vmn0000'), ('194', 'Z'), ('millones', 'ncmp000'), ('de', 'sps00'), ('euros', 'Zm'), ('-Fpa-', 'Fpa'), ('186', 'Z'), ('millones', 'ncmp000'), ('de', 'sps00'), ('dólares', 'Zm'), ('-Fpt-', 'Fpt'), ('en', 'sps00'), ('la', 'da0fs0'), ('central', 'ncfs000'), ('de', 'sps00'), ('Río_Bravo', 'np00000'), (',', 'Fc'), ('con', 'sps00'), ('una', 'di0fs0'), ('potencia', 'ncfs000'), ('de', 'sps00'), ('495', 'Z'), ('megavatios', 'ncmp000'), (',', 'Fc'), ('y', 'cc'), ('134', 'Z'), ('millones', 'ncmp000'), ('de', 'sps00'), ('euros', 'Zm'), ('-Fpa-', 'Fpa'), ('28', 'Z'), ('millones', 'ncmp000'), ('de', 'sps00'), ('dólares', 'Zm'), ('-Fpt-', 'Fpt'), ('en', 'sps00'), ('Saltillo', 'np00000'), (',', 'Fc'), ('que', 'pr0cn000'), ('como', 'cs'), ('la', 'da0fs0'), ('primera', 'ao0fs0'), ('funcionará', 'vmif3s0'), ('con', 'sps00'), ('gas', 'ncms000'), ('natural', 'aq0cs0'), ('y', 'cc'), ('cuya', 'pr0fs000'), ('potencia', 'ncfs000'), ('prevista', 'aq0fsp'), ('es', 'vsip3s0'), ('de', 'sps00'), ('247', 'Z'), ('megavatios', 'ncmp000'), ('.', 'Fp')]\n"
     ]
    }
   ],
   "source": [
    "# Los resultados en spanish no son muy alentadores :(\n",
    "\n",
    "# En el siguiente enlace explica como importar un corpus en spanish\n",
    "# https://notebook.community/vitojph/kschool-nlp/notebooks-py3/pos-tagger-es\n",
    "\n",
    "from nltk.corpus import cess_esp\n",
    "cess_esp = cess_esp.tagged_sents()\n",
    "print(cess_esp[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635400c5",
   "metadata": {},
   "source": [
    "Notamos que las etiquetas que se usan en el treebank español son diferentes a las etiquetas que habíamos visto en inglés. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
